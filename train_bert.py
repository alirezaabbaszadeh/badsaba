# train_bert.py
from pathlib import Path
import numpy as np
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# âœ¨ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
from src.config import config
from src.data_manager_bert import DataManagerBERT 
from src.model_pipeline_bert import create_bert_model
from src.utils import logger

from pathlib import Path
import numpy as np
# âœ¨ Û±. ReduceLROnPlateau Ø±Ø§ Ø§Ø² keras.callbacks ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from src.model_pipeline_bert import create_bert_model


def run_bert_training():
    logger.info("="*20 + " Starting BERT Model Training Pipeline " + "="*20)

    # 1. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² DataManagerBERT
    data_manager = DataManagerBERT(config=config)

    # 2. Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³Ø§Ø®Øª ÙˆÚ©ØªÙˆØ±Ø§ÛŒØ²Ø± Ù†ÛŒØ³Øª)
    logger.info("Preparing training data for the BERT model...")
    X_train, y_train = data_manager.get_data_for_model(df=data_manager.p_train_df)
    
    # 3. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ BERT
  


    # âœ¨ ØªØºÛŒÛŒØ±: Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ùˆ Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù† Ø¢Ù† Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø³Ø§Ø®Øª Ù…Ø¯Ù„
    num_extra_features = X_train[2].shape[1] # X_train[2] Ù‡Ù…Ø§Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒâ€ŒØ´Ø¯Ù‡ Ø§Ø³Øª

    logger.info(f"Number of engineered features: {num_extra_features}")
    # --- Û´. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ BERT ---
    # model = create_bert_model(config=config)
    model = create_bert_model(config=config, num_extra_features=num_extra_features)

    
    # --- Ûµ. ØªØ¹Ø±ÛŒÙ Callbacks Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ ---
    artifacts_dir = Path(config['artifact_paths']['artifacts_dir'])
    model_path = artifacts_dir / "preference_model_bert.h5" 
    
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)
    
    # âœ¨ Û². ØªØ¹Ø±ÛŒÙ Ú©Ø±Ø¯Ù† ReduceLROnPlateau
    # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (val_loss) Ø¨Ø¹Ø¯ Ø§Ø² 2 Ø¯ÙˆØ± Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯ØŒ
    # Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±Ø§ Ø¨Ø§ Ø¶Ø±ÛŒØ¨ 0.2 à¦—à§à¦£ Ú©Ù† (ÛŒØ¹Ù†ÛŒ 80% Ú©Ø§Ù‡Ø´ Ø¨Ø¯Ù‡).
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.1,             # Ø¶Ø±ÛŒØ¨ Ú©Ø§Ù‡Ø´: new_lr = lr * factor
        patience=1,             # ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡Ø§ÛŒ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯
        min_lr=0.00001,         # Ø­Ø¯Ø§Ù‚Ù„ Ù…Ù‚Ø¯Ø§Ø± Ù…Ù…Ú©Ù† Ø¨Ø±Ø§ÛŒ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        verbose=1               # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ù‡Ù†Ú¯Ø§Ù… Ú©Ø§Ù‡Ø´ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
    )
    
    # --- Û¶. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---
    logger.info("ðŸš€ Starting BERT model training...")
    model_params = config['model_params']
    history = model.fit(
        x=X_train,
        y=y_train,
        batch_size=model_params['batch_size'],
        epochs=model_params['epochs'],
        validation_split=model_params['validation_split'],
        # âœ¨ Û³. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† callback Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù„ÛŒØ³Øª
        callbacks=[early_stopping, model_checkpoint, reduce_lr] 
    )


    # ... (Ø¨Ø®Ø´ Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„)
    best_epoch_idx = np.argmin(history.history['val_loss'])
    best_val_r2 = history.history['val_r_squared'][best_epoch_idx]
    logger.info(f"ðŸ† Best BERT model saved to: {model_path}")
    logger.info(f"  -> Best Validation R-squared: {best_val_r2:.4f}")
    logger.info("="*20 + " BERT Model Training Pipeline Finished " + "="*20)

if __name__ == '__main__':
    run_bert_training()